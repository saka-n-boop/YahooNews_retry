import os
import re
import time
import json
import gspread
import requests
import traceback
import google.generativeai as genai
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
from google.oauth2.service_account import Credentials
from google.api_core.exceptions import GoogleAPIError
from gspread.exceptions import APIError as GSpreadAPIError

# --- ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•° ---
# Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã®ã‚¹ã‚³ãƒ¼ãƒ—ã¨èªè¨¼æƒ…å ±
SCOPES = [
    "https://www.googleapis.com/auth/spreadsheets",
    "https://www.googleapis.com/auth/drive",
]
# ç’°å¢ƒå¤‰æ•°ã‹ã‚‰ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‚­ãƒ¼ã‚’å–å¾—
SPREADSHEET_KEY = os.environ.get("SPREADSHEET_KEY")
if not SPREADSHEET_KEY:
    # ãƒ­ã‚°å‡ºåŠ›ï¼ˆè‡´å‘½çš„ã‚¨ãƒ©ãƒ¼ï¼‰
    print("âŒ ç’°å¢ƒå¤‰æ•° 'SPREADSHEET_KEY' ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚", flush=True)
    # ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å®Ÿè¡Œã‚’åœæ­¢
    exit(1) # exit(1) ã¯ã‚¨ãƒ©ãƒ¼ã«ã‚ˆã‚‹çµ‚äº†ã‚’ç¤ºã™

# Geminiãƒ¢ãƒ‡ãƒ«ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
gemini_model = None

# æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
SEARCH_KEYWORDS = [
    "ãƒˆãƒ¨ã‚¿", "æ—¥ç”£", "ãƒ›ãƒ³ãƒ€", "ä¸‰è±è‡ªå‹•è»Š",
    "ãƒãƒ„ãƒ€", "ã‚¹ãƒãƒ«", "ãƒ€ã‚¤ãƒãƒ„", "ã‚¹ã‚ºã‚­"
]

# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
PROMPT_FILES = {
    "role": "prompt_gemini_role.txt",
    "sentiment": "prompt_posinega.txt",
    "category": "prompt_category.txt",
    "company_info": "prompt_target_company.txt",
    "nissan_mention": "prompt_nissan_mention.txt",
    "nissan_sentiment": "prompt_nissan_sentiment.txt",
}

# èª­ã¿è¾¼ã‚“ã ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ ¼ç´ã™ã‚‹è¾æ›¸
PROMPTS = {}


def setup_gspread():
    """
    Google ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ API ã¸ã®èªè¨¼ã‚’è¡Œã†ã€‚
    ç’°å¢ƒå¤‰æ•° GCP_SERVICE_ACCOUNT_KEY ã‹ã‚‰èªè¨¼æƒ…å ±ã‚’èª­ã¿è¾¼ã‚€ã€‚
    """
    try:
        # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚­ãƒ¼ã®JSONæ–‡å­—åˆ—ã‚’å–å¾—
        creds_json_str = os.environ.get("GCP_SERVICE_ACCOUNT_KEY")
        if not creds_json_str:
            print("âŒ ç’°å¢ƒå¤‰æ•° 'GCP_SERVICE_ACCOUNT_KEY' ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
            return None

        # JSONæ–‡å­—åˆ—ã‚’è¾æ›¸ã«å¤‰æ›
        creds_dict = json.loads(creds_json_str)

        # è¾æ›¸ã‹ã‚‰èªè¨¼æƒ…å ±ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
        credentials = Credentials.from_service_account_info(creds_dict, scopes=SCOPES)
        
        # gspread ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’èªè¨¼
        gc = gspread.authorize(credentials)
        
        # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆãŒé–‹ã‘ã‚‹ã‹ãƒ†ã‚¹ãƒˆ
        gc.open_by_key(SPREADSHEET_KEY)
        
        print("âœ… Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã¸ã®èªè¨¼ã«æˆåŠŸã—ã¾ã—ãŸã€‚")
        return gc

    except json.JSONDecodeError:
        print("âŒ 'GCP_SERVICE_ACCOUNT_KEY' ã®JSONå½¢å¼ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“ã€‚")
        return None
    except Exception as e:
        print(f"âŒ Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã¸ã®èªè¨¼ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return None


def get_worksheet(gc, sheet_name):
    """
    gspread ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã¨ã‚·ãƒ¼ãƒˆåã‚’å—ã‘å–ã‚Šã€ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ¼ãƒˆã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’è¿”ã™ã€‚
    """
    if not gc:
        print(f"  âŒ ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ¼ãƒˆ '{sheet_name}' ã‚’å–å¾—ã§ãã¾ã›ã‚“ (gspreadã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæœªåˆæœŸåŒ–)ã€‚")
        return None
    try:
        spreadsheet = gc.open_by_key(SPREADSHEET_KEY)
        worksheet = spreadsheet.worksheet(sheet_name)
        return worksheet
    except GSpreadAPIError as e:
        print(f"  âŒ ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ¼ãƒˆ '{sheet_name}' ãŒè¦‹ã¤ã‹ã‚‰ãªã„ã‹ã€ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ãŒã‚ã‚Šã¾ã›ã‚“: {e}")
        return None
    except Exception as e:
        print(f"  âŒ ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ¼ãƒˆ '{sheet_name}' ã®å–å¾—ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {e}")
        return None


def load_existing_urls(ws):
    """
    SOURCE ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ¼ãƒˆã‹ã‚‰ B åˆ—ï¼ˆURLï¼‰ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã€
    é‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨ã®ã‚»ãƒƒãƒˆã¨ã—ã¦è¿”ã™ã€‚
    """
    try:
        # Båˆ—ã®å…¨ã¦ã®å€¤ã‚’å–å¾—
        urls = ws.col_values(2) # Båˆ—ã¯ 2
        # 1è¡Œç›®ï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼ï¼‰ã‚’é™¤ã
        return set(urls[1:])
    except Exception as e:
        print(f"  âŒ æ—¢å­˜URLã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        # ç©ºã®ã‚»ãƒƒãƒˆã‚’è¿”ã—ã¦å‡¦ç†ã‚’ç¶šè¡Œ
        return set()


# (ä¿®æ­£æ¸ˆ) Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®HTMLæ§‹é€ å¤‰æ›´ï¼ˆä¸€è¦§ãƒšãƒ¼ã‚¸ï¼‰ã«å¯¾å¿œ
def get_yahoo_news_search_results(keyword):
    """
    æŒ‡å®šã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æ¤œç´¢ã—ã€
    è¨˜äº‹ã®ã‚¿ã‚¤ãƒˆãƒ«ã€URLã€ç™ºè¡Œå…ƒã€æŠ•ç¨¿æ™‚é–“ã®ãƒªã‚¹ãƒˆã‚’è¿”ã™ã€‚
    """
    print(f"  Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¤œç´¢é–‹å§‹ (ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {keyword})...")
    search_url = f"https://news.yahoo.co.jp/search?p={keyword}&ei=utf-8"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }
    
    try:
        response = requests.get(search_url, headers=headers)
        response.raise_for_status() # HTTPã‚¨ãƒ©ãƒ¼ã‚’ãƒã‚§ãƒƒã‚¯
        
        soup = BeautifulSoup(response.text, "html.parser")
        
        # --- ã‚³ãƒ³ãƒ†ãƒŠã‚’æ¢ã™ ---
        # (æ–°) <ol class="newsFeed_list"> ã‚’æ¢ã™
        search_results_container = soup.find("ol", class_="newsFeed_list")
        # (æ–°) <div class="newsFeed"> (å°æ–‡å­—) ã‚’æ¢ã™
        if not search_results_container:
            search_results_container = soup.find("div", class_="newsFeed")
        # (æ—§) <div class="NewsFeed"> (å¤§æ–‡å­—) ã‚’æ¢ã™
        if not search_results_container:
             search_results_container = soup.find("div", class_="NewsFeed")
        # (æ—§) <div class...="Search__ResultList"> ã‚’æ¢ã™
        if not search_results_container:
            search_results_container = soup.find("div", class_=re.compile(r"Search__ResultList"))

        if not search_results_container:
            print(f"  - æ¤œç´¢çµæœã®ã‚³ãƒ³ãƒ†ãƒŠãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ (ol.newsFeed_list, div.newsFeed, div.NewsFeed, Search__ResultList ã®ã„ãšã‚Œã‹)ã€‚")
            return []

        # --- è¨˜äº‹è¦ç´  (li) ã‚’æ¢ã™ ---
        articles = search_results_container.find_all("li")
        if not articles:
            articles = search_results_container.find_all("div", class_="newsFeed_item")

        if not articles:
            print("  - è¨˜äº‹è¦ç´  (li or div.newsFeed_item) ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            return []

        results = []
        for article in articles:
            try:
                # --- è¨˜äº‹ã®ã€Œæœ¬æ–‡ã€é ˜åŸŸã®ã‚¯ãƒ©ã‚¹ã‚’ã‚¢ãƒ³ã‚«ãƒ¼ã«ã™ã‚‹ ---
                body_tag = article.find("div", class_="newsFeed_item_body")
                
                # body ãŒãªã„ (åºƒå‘Šliãªã©) å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                if not body_tag:
                    continue

                # body ã‹ã‚‰è¦ªã® <a> ã‚¿ã‚°ã‚’æ¢ã—ã¦ URL ã‚’å–å¾—
                title_tag = body_tag.find_parent("a")
                
                if not title_tag or "href" not in title_tag.attrs:
                    continue 

                url = title_tag["href"]
                
                # è¨˜äº‹URLä»¥å¤–ã¯é™¤å¤–
                if not url.startswith("https://news.yahoo.co.jp/articles/"):
                    continue

                # --- ã‚¿ã‚¤ãƒˆãƒ«ã€ç™ºè¡Œå…ƒã€æ™‚é–“ã‚’å–å¾— ---
                title = "ï¼ˆã‚¿ã‚¤ãƒˆãƒ«å–å¾—å¤±æ•—ï¼‰"
                source = "ç™ºè¡Œå…ƒä¸æ˜"
                post_time_str = "æ™‚é–“ä¸æ˜"

                # time ã‚¿ã‚°ã‚’æ¢ã™
                time_tag = body_tag.find("time")
                if time_tag:
                    post_time_str = time_tag.text.strip()
                    
                    # time ã‚¿ã‚°ã®è¦ªã‹ã‚‰ span (ç™ºè¡Œå…ƒ) ã‚’æ¢ã™
                    meta_container = time_tag.find_parent("div")
                    if meta_container:
                        source_tag = meta_container.find("span")
                        if source_tag:
                            source = source_tag.text.strip()

                # ã‚¿ã‚¤ãƒˆãƒ«ã‚’æ¢ã™ (å‹•çš„ã‚¯ãƒ©ã‚¹å `sc-` ã«ä¾å­˜ã—ãªã„æ–¹æ³•)
                # 'newsFeed_item_body' ã®ä¸­ã«ã‚ã‚‹ 'a' ã‚¿ã‚°ã® 'div' ã§ã‚¯ãƒ©ã‚¹åãŒ 'sc-' ã§å§‹ã¾ã‚‹ã‚‚ã®ã‚’æ¢ã™
                title_text_tag = body_tag.find("div", class_=re.compile(r"^sc-3ls169-0")) # æš«å®šçš„ãªç›®å°
                
                if not title_text_tag:
                    # 'sc-' ã§å§‹ã¾ã‚‹ã‚¯ãƒ©ã‚¹ã‚’æŒã¤ div ã‚’å…¨ã¦æ¢ã—ã€ãã®ä¸­ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆã™ã‚‹ (å …ç‰¢æ€§ã‚’é«˜ã‚ã‚‹)
                    title_divs = body_tag.select("div[class*='sc-']")
                    if title_divs:
                        # æœ€åˆã® 'sc-' ã‚¯ãƒ©ã‚¹ã® div ã‚’ã‚¿ã‚¤ãƒˆãƒ«ã¨ã™ã‚‹
                        title = title_divs[0].get_text(strip=True)

                if title_text_tag and title == "ï¼ˆã‚¿ã‚¤ãƒˆãƒ«å–å¾—å¤±æ•—ï¼‰":
                        title = title_text_tag.get_text(strip=True)

                # <em> ã‚¿ã‚°å†…ã®ãƒ†ã‚­ã‚¹ãƒˆã‚‚å–å¾—ï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒãƒã‚¤ãƒ©ã‚¤ãƒˆã•ã‚Œã¦ã„ã‚‹å ´åˆï¼‰
                if title == "ï¼ˆã‚¿ã‚¤ãƒˆãƒ«å–å¾—å¤±æ•—ï¼‰" and title_tag.find("div", class_=re.compile(r"newsFeed_item_title")):
                     title = title_tag.find("div", class_=re.compile(r"newsFeed_item_title")).get_text(strip=True)
                
                if title == "ï¼ˆã‚¿ã‚¤ãƒˆãƒ«å–å¾—å¤±æ•—ï¼‰":
                    # æœ€çµ‚æ‰‹æ®µ
                    title = title_tag.get_text(strip=True).split("\n")[0]


                results.append({
                    "title": title,
                    "url": url,
                    "source": source,
                    "post_time_str": post_time_str,
                    "keyword": keyword
                })

            except Exception as e:
                print(f"  - è¨˜äº‹ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
                continue
                
        print(f"  Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ä»¶æ•°: {len(results)} ä»¶å–å¾—")
        return results

    except requests.exceptions.RequestException as e:
        print(f"  âŒ Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¤œç´¢ãƒªã‚¯ã‚¨ã‚¹ãƒˆå¤±æ•—: {e}")
        return []
    except Exception as e:
        print(f"  âŒ Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¤œç´¢å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        traceback.print_exc()
        return []


def parse_relative_time(time_str):
    """
    Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ç›¸å¯¾æ™‚é–“ï¼ˆä¾‹: '1æ™‚é–“å‰', '11/11(æœˆ) 10:00'ï¼‰ã‚’
    datetime ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›ã™ã‚‹ã€‚
    """
    now = datetime.now()
    
    # 1. '11/11(æœˆ) 10:00' å½¢å¼ (ä»Šå¹´)
    match = re.search(r"(\d{1,2})/(\d{1,2})\(.\) (\d{1,2}):(\d{1,2})", time_str)
    if match:
        month, day, hour, minute = map(int, match.groups())
        try:
            return now.replace(month=month, day=day, hour=hour, minute=minute, second=0, microsecond=0)
        except ValueError:
             return now.replace(year=now.year - 1, month=month, day=day, hour=hour, minute=minute, second=0, microsecond=0)

    # 2. 'â—‹åˆ†å‰' å½¢å¼
    match = re.search(r"(\d+)åˆ†å‰", time_str)
    if match:
        minutes = int(match.group(1))
        return now - timedelta(minutes=minutes)

    # 3. 'â—‹æ™‚é–“å‰' å½¢å¼
    match = re.search(r"(\d+)æ™‚é–“å‰", time_str)
    if match:
        hours = int(match.group(1))
        return now - timedelta(hours=hours)

    # 4. 'æ˜¨æ—¥' å½¢å¼
    if "æ˜¨æ—¥" in time_str:
        match = re.search(r"(\d{1,2}):(\d{1,2})", time_str)
        day_delta = 1
        if match:
            hour, minute = map(int, match.groups())
            return (now - timedelta(days=day_delta)).replace(hour=hour, minute=minute, second=0, microsecond=0)
        else:
            return now - timedelta(days=day_delta)

    # 5. 'â—‹æ—¥å‰' å½¢å¼
    match = re.search(r"(\d+)æ—¥å‰", time_str)
    if match:
        days = int(match.group(1))
        return now - timedelta(days=days)

    # ä¸æ˜ãªå½¢å¼
    return None


# --- (ä¿®æ­£ç®‡æ‰€) ---
# è¨˜äº‹æœ¬æ–‡ãƒšãƒ¼ã‚¸ã®HTMLæ§‹é€ å¤‰æ›´ã«å¯¾å¿œ
def get_article_details(article_url):
    """
    è¨˜äº‹URLã‹ã‚‰æœ¬æ–‡ï¼ˆæœ€å¤§10ãƒšãƒ¼ã‚¸ï¼‰ã€ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã€æ­£ç¢ºãªæŠ•ç¨¿æ—¥æ™‚ã‚’å–å¾—ã™ã‚‹ã€‚
    """
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }
    
    article_body_parts = []
    comment_count = "0" # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    full_post_time = None # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ

    try:
        # --- 1ãƒšãƒ¼ã‚¸ç›®ã®å–å¾— (ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã¨æ—¥æ™‚ã‚‚ã“ã“ã‹ã‚‰å–ã‚‹) ---
        response = requests.get(article_url, headers=headers)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")

        # ã‚³ãƒ¡ãƒ³ãƒˆæ•° (å‹•çš„ã‚¯ãƒ©ã‚¹åå¯¾å¿œ)
        comment_count_tag = soup.find("a", class_=re.compile(r"CommentCount__CommentCountButton"), href=re.compile(r"/comments/"))
        if not comment_count_tag:
            # (ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯) sc-1n9vtw0-1 (ã‚³ãƒ¡ãƒ³ãƒˆãƒœã‚¿ãƒ³)
            comment_count_tag = soup.find("button", class_=re.compile(r"sc-1n9vtw0-1"))
        
        if comment_count_tag:
            match = re.search(r"(\d+)", comment_count_tag.text)
            if match:
                comment_count = match.group(1)

        # æ­£ç¢ºãªæŠ•ç¨¿æ—¥æ™‚
        time_tag = soup.find("time")
        if time_tag and time_tag.has_attr("datetime"):
            try:
                full_post_time = datetime.fromisoformat(time_tag["datetime"].replace("Z", "+00:00"))
            except ValueError:
                print(f"  - æ—¥æ™‚ãƒ‘ãƒ¼ã‚¹å¤±æ•—: {time_tag['datetime']}")
                full_post_time = None

        # --- (ä¿®æ­£) è¨˜äº‹æœ¬æ–‡ (1ãƒšãƒ¼ã‚¸ç›®) ---
        # æ—§: class_=re.compile(r"ArticleBody")
        # æ–°: class="article_body"
        body_container = soup.find("div", class_="article_body")
        
        if body_container:
            # æœ¬æ–‡ãƒ†ã‚­ã‚¹ãƒˆ
            body_text = body_container.get_text(separator="\n", strip=True)
            article_body_parts.append(body_text)
        else:
            print(f"  - è¨˜äº‹æœ¬æ–‡(P1)ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ (URL: {article_url})")
            article_body_parts.append("ï¼ˆæœ¬æ–‡å–å¾—å¤±æ•—ï¼‰")


        # --- 2ãƒšãƒ¼ã‚¸ç›®ä»¥é™ã®å–å¾— (æœ€å¤§10ãƒšãƒ¼ã‚¸) ---
        for page_num in range(2, 11): # 2ã€œ10ãƒšãƒ¼ã‚¸
            next_page_url = f"{article_url}?page={page_num}"
            try:
                response_page = requests.get(next_page_url, headers=headers)
                
                if response_page.status_code != 200:
                    print(f"  - è¨˜äº‹æœ¬æ–‡ ãƒšãƒ¼ã‚¸ {page_num} ã¯å­˜åœ¨ã—ã¾ã›ã‚“ã§ã—ãŸã€‚æœ¬æ–‡å–å¾—ã‚’å®Œäº†ã—ã¾ã™ã€‚")
                    break 
                
                soup_page = BeautifulSoup(response_page.text, "html.parser")
                # --- (ä¿®æ­£) 2ãƒšãƒ¼ã‚¸ç›®ä»¥é™ã®æœ¬æ–‡ ---
                body_container_page = soup_page.find("div", class_="article_body")
                
                if body_container_page:
                    body_text_page = body_container_page.get_text(separator="\n", strip=True)
                    if body_text_page == article_body_parts[0]:
                         print(f"  - è¨˜äº‹æœ¬æ–‡ ãƒšãƒ¼ã‚¸ {page_num} ã¯1ãƒšãƒ¼ã‚¸ç›®ã¨åŒã˜å†…å®¹ã®ãŸã‚çµ‚äº†ã—ã¾ã™ã€‚")
                         break
                    
                    print(f"  - è¨˜äº‹æœ¬æ–‡ ãƒšãƒ¼ã‚¸ {page_num} ã‚’å–å¾—ã—ã¾ã—ãŸã€‚")
                    article_body_parts.append(body_text_page)
                else:
                    print(f"  - è¨˜äº‹æœ¬æ–‡ ãƒšãƒ¼ã‚¸ {page_num} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                    break
                
                time.sleep(1) 

            except requests.exceptions.RequestException as re_e:
                if "404" in str(re_e):
                    print(f"  âŒ ãƒšãƒ¼ã‚¸ãªã— (404 Client Error): {next_page_url}")
                    print(f"  - è¨˜äº‹æœ¬æ–‡ ãƒšãƒ¼ã‚¸ {page_num} ã¯å­˜åœ¨ã—ã¾ã›ã‚“ã§ã—ãŸã€‚æœ¬æ–‡å–å¾—ã‚’å®Œäº†ã—ã¾ã™ã€‚")
                else:
                    print(f"  âŒ ãƒšãƒ¼ã‚¸ {page_num} å–å¾—ã‚¨ãƒ©ãƒ¼: {re_e}")
                break
            except Exception as e_page:
                print(f"  âŒ ãƒšãƒ¼ã‚¸ {page_num} å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e_page}")
                break

    except requests.exceptions.RequestException as re_e:
        print(f"  âŒ è¨˜äº‹è©³ç´°ãƒšãƒ¼ã‚¸å–å¾—ã‚¨ãƒ©ãƒ¼ (URL: {article_url}): {re_e}")
        return ["ï¼ˆæœ¬æ–‡å–å¾—å¤±æ•—ï¼‰"] * 10, "0", None
    except Exception as e:
        print(f"  âŒ è¨˜äº‹è©³ç´°å‡¦ç†ã‚¨ãƒ©ãƒ¼ (URL: {article_url}): {e}")
        traceback.print_exc()
        return ["ï¼ˆæœ¬æ–‡å–å¾—å¤±æ•—ï¼‰"] * 10, "0", None

    if len(article_body_parts) < 10:
        article_body_parts.extend(["-"] * (10 - len(article_body_parts)))
    
    return article_body_parts[:10], comment_count, full_post_time


def load_prompts():
    """
    ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€ã€‚
    """
    global PROMPTS
    print("  ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™...")
    try:
        for key, file_path in PROMPT_FILES.items():
            if not os.path.exists(file_path):
                print(f"  âŒ è­¦å‘Š: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ« '{file_path}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                continue
                
            with open(file_path, "r", encoding="utf-8") as f:
                PROMPTS[key] = f.read()
        
        if not PROMPTS:
             print("  âŒ ã‚¨ãƒ©ãƒ¼: èª­ã¿è¾¼ã‚ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒ1ã¤ã‚‚ã‚ã‚Šã¾ã›ã‚“ã€‚")
             return False
             
        print("  âœ… ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®èª­ã¿è¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
        return True

    except Exception as e:
        print(f"  âŒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        return False


def initialize_gemini():
    """
    Gemini API ã‚’åˆæœŸåŒ–ã™ã‚‹ã€‚
    """
    global gemini_model
    try:
        api_key = os.environ.get("GOOGLE_API_KEY")
        if not api_key:
            print("  âŒ è­¦å‘Š: ç’°å¢ƒå¤‰æ•° 'GOOGLE_API_KEY' ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
            return

        if hasattr(genai, "configure"):
             genai.configure(api_key=api_key)
        else:
             print("  âš ï¸ è­¦å‘Š: genai.configure ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚APIã‚­ãƒ¼ã®æ‰‹å‹•è¨­å®šã‚’è©¦ã¿ã¾ã™ã€‚")
             pass 
        
        model = genai.GenerativeModel('gemini-pro')
        
        if not hasattr(genai, "configure"):
            model = genai.GenerativeModel('gemini-pro', api_key=api_key)

        gemini_model = model
        print("âœ… Geminiã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–ã«æˆåŠŸã—ã¾ã—ãŸã€‚ (model: gemini-pro)")

    except Exception as e:
        print(f"  âŒ è­¦å‘Š: Geminiã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚Geminiåˆ†æã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã™ã€‚ã‚¨ãƒ©ãƒ¼: {e}")
        traceback.print_exc()
        gemini_model = None


def analyze_article_with_gemini(article_body):
    """
    è¨˜äº‹æœ¬æ–‡ã‚’å—ã‘å–ã‚Šã€Gemini API ã§åˆ†æã™ã‚‹ã€‚
    """
    if not gemini_model:
        return {
            "sentiment": "N/A", "category": "N/A", "company_info": "N/A",
            "nissan_mention": "N/A", "nissan_sentiment": "N/A"
        }

    max_length = 10000
    if len(article_body) > max_length:
        article_body = article_body[:max_length]

    full_prompt = f"""
{PROMPTS.get("role", "ã‚ãªãŸã¯æ¥­ç•Œã‚¢ãƒŠãƒªã‚¹ãƒˆã§ã™ã€‚")}

ã€è¨˜äº‹æœ¬æ–‡ã€‘
{article_body}
ã€è¨˜äº‹æœ¬æ–‡ã“ã“ã¾ã§ã€‘

---
ã€ã‚¿ã‚¹ã‚¯ã€‘
è¨˜äº‹æœ¬æ–‡ã‚’åˆ†æã—ã€ä»¥ä¸‹ã®ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚
çµæœã¯å¿…ãšæŒ‡å®šã•ã‚ŒãŸJSONãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§ã€ã‚­ãƒ¼ã€Œsentimentã€ã€Œcategoryã€ã€Œcompany_infoã€ã€Œnissan_mentionã€ã€Œnissan_sentimentã€ã‚’æŒã¤å˜ä¸€ã®JSONã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¨ã—ã¦å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

1. **sentimentã®åˆ¤å®š**:
{PROMPTS.get("sentiment", "ï¼ˆsentimentãƒ«ãƒ¼ãƒ«ï¼‰")}

2. **categoryã®åˆ¤å®š**:
{PROMPTS.get("category", "ï¼ˆcategoryãƒ«ãƒ¼ãƒ«ï¼‰")}

3. **company_infoã®åˆ¤å®š**:
{PROMPTS.get("company_info", "ï¼ˆcompany_infoãƒ«ãƒ¼ãƒ«ï¼‰")}

4. **nissan_mentionã®åˆ¤å®š**:
(æ³¨: company_infoãŒã€Œæ—¥ç”£ã€*ä»¥å¤–*ã®å ´åˆã®ã¿ã€æœ¬æ–‡ä¸­ã®ã€Œæ—¥ç”£ã€ã¸ã®è¨€åŠã‚’ç¢ºèªã›ã‚ˆ)
{PROMPTS.get("nissan_mention", "ï¼ˆnissan_mentionãƒ«ãƒ¼ãƒ«ï¼‰")}

5. **nissan_sentimentã®åˆ¤å®š**:
(æ³¨: nissan_mentionãŒã€Œ-ã€*ä»¥å¤–*ã®å ´åˆã®ã¿ã€ãã®è¨€åŠãŒæ—¥ç”£ã«ã¨ã£ã¦ãƒã‚¸ãƒ†ã‚£ãƒ–/ãƒã‚¬ãƒ†ã‚£ãƒ–/ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«ã‹åˆ¤å®šã›ã‚ˆ)
{PROMPTS.get("nissan_sentiment", "ï¼ˆnissan_sentimentãƒ«ãƒ¼ãƒ«ï¼‰")}

---
ã€å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ (JSON)ã€‘
{{
  "sentiment": "ï¼ˆ1ã®åˆ¤å®šçµæœï¼‰",
  "category": "ï¼ˆ2ã®åˆ¤å®šçµæœï¼‰",
  "company_info": "ï¼ˆ3ã®åˆ¤å®šçµæœï¼‰",
  "nissan_mention": "ï¼ˆ4ã®åˆ¤å®šçµæœï¼‰",
  "nissan_sentiment": "ï¼ˆ5ã®åˆ¤å®šçµæœï¼‰"
}}
"""

    try:
        response = gemini_model.generate_content(full_prompt)
        
        json_match = re.search(r"\{.*\}", response.text, re.DOTALL)
        
        if not json_match:
            print("  âŒ Geminiå¿œç­”ã‹ã‚‰JSONã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
            print(f"  å¿œç­”: {response.text}")
            return {
                "sentiment": "N/A", "category": "N/A", "company_info": "N/A",
                "nissan_mention": "N/A", "nissan_sentiment": "N/A"
            }

        json_str = json_match.group(0)
        result = json.loads(json_str)
        
        required_keys = ["sentiment", "category", "company_info", "nissan_mention", "nissan_sentiment"]
        if not all(key in result for key in required_keys):
             print(f"  âŒ Geminiå¿œç­”JSONã«å¿…è¦ãªã‚­ãƒ¼ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚ {result.keys()}")
             for key in required_keys:
                 if key not in result:
                     result[key] = "N/A (ã‚­ãƒ¼æ¬ æ)"

        return result

    except json.JSONDecodeError as e:
        print(f"  âŒ Geminiå¿œç­”ã®JSONãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        print(f"  å¿œç­”ãƒ†ã‚­ã‚¹ãƒˆ (JSONæŠ½å‡ºå¾Œ): {json_str}")
        return {
            "sentiment": "N/A", "category": "N/A", "company_info": "N/A",
            "nissan_mention": "N/A", "nissan_sentiment": "N/A"
        }
    except GoogleAPIError as e:
        print(f"  âŒ Gemini API ã‚¨ãƒ©ãƒ¼: {e}")
        return {
            "sentiment": "N/A", "category": "N/A", "company_info": "N/A",
            "nissan_mention": "N/A", "nissan_sentiment": "N/A"
        }
    except Exception as e:
        print(f"  âŒ Geminiåˆ†æä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {e}")
        traceback.print_exc()
        return {
            "sentiment": "N/A", "category": "N/A", "company_info": "N/A",
            "nissan_mention": "N/A", "nissan_sentiment": "N/A"
        }


# --- (ä¿®æ­£ç®‡æ‰€) ---
# ã‚³ãƒ¡ãƒ³ãƒˆæ¬„ã®HTMLæ§‹é€ å¤‰æ›´ï¼ˆå‹•çš„ã‚¯ãƒ©ã‚¹åï¼‰ã«å¯¾å¿œ
def get_yahoo_news_comments(article_id, article_url):
    """
    è¨˜äº‹IDã¨è¨˜äº‹URLã‚’å—ã‘å–ã‚Šã€ã‚³ãƒ¡ãƒ³ãƒˆãƒšãƒ¼ã‚¸ã®1ã€œ3ãƒšãƒ¼ã‚¸ç›®ã¾ã§ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã™ã‚‹ã€‚
    (å‹•çš„ãª `sc-` ã‚¯ãƒ©ã‚¹åã«å¯¾å¿œ)
    """
    print(f"    - ã‚³ãƒ¡ãƒ³ãƒˆæœ¬æ–‡ (Såˆ—ï½ACåˆ—) ã‚’å–å¾—ä¸­...")
    comments_data = []
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }

    try:
        base_comments_url = f"{article_url}/comments"
        
        for page_num in range(1, 4): # 1ãƒšãƒ¼ã‚¸ã‹ã‚‰3ãƒšãƒ¼ã‚¸ã¾ã§
            if page_num == 1:
                comments_url = base_comments_url
            else:
                comments_url = f"{base_comments_url}?page={page_num}"

            response = requests.get(comments_url, headers=headers)
            
            if response.status_code != 200:
                print(f"    âŒ ã‚³ãƒ¡ãƒ³ãƒˆ ãƒšãƒ¼ã‚¸ {page_num} ( {comments_url} ) ãŒå­˜åœ¨ã—ãªã„ã‹å–å¾—å¤±æ•—ã€‚ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {response.status_code}")
                break 

            soup = BeautifulSoup(response.text, "html.parser")

            # --- (ä¿®æ­£) å‹•çš„ã‚¯ãƒ©ã‚¹åå¯¾å¿œ ---
            # 1. ã‚³ãƒ¡ãƒ³ãƒˆæ¬„ã®ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒŠã‚’æ¢ã™
            comment_main = soup.find("article", id="comment-main")
            if not comment_main:
                 print(f"    - ã‚³ãƒ¡ãƒ³ãƒˆ ãƒšãƒ¼ã‚¸ {page_num} ã« 'comment-main' ã‚³ãƒ³ãƒ†ãƒŠãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                 break

            # 2. ã‚³ãƒ³ãƒ†ãƒŠå†…ã®å…¨ <article> ã‚¿ã‚° (ã“ã‚ŒãŒå„ã‚³ãƒ¡ãƒ³ãƒˆ) ã‚’æ¢ã™
            #    (å°‚é–€å®¶ã‚³ãƒ¡ãƒ³ãƒˆ `sc-z8tf0-1`ã€ä¸€èˆ¬ã‚³ãƒ¡ãƒ³ãƒˆ `sc-169yn8p-3` ã«å¯¾å¿œ)
            comments = comment_main.find_all("article", class_=re.compile(r"sc-"))
            
            if not comments:
                # print(f"    - ã‚³ãƒ¡ãƒ³ãƒˆ ãƒšãƒ¼ã‚¸ {page_num} ã«ã‚³ãƒ¡ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                break 

            for comment in comments:
                user_name = "ãƒ¦ãƒ¼ã‚¶ãƒ¼åä¸æ˜"
                comment_text = "ã‚³ãƒ¡ãƒ³ãƒˆæœ¬æ–‡ãªã—"
                
                # 3. ãƒ¦ãƒ¼ã‚¶ãƒ¼å (h2 ã‚¿ã‚°) ã‚’æ¢ã™
                user_name_tag = comment.find("h2")
                if user_name_tag:
                    user_name = user_name_tag.get_text(strip=True)

                # 4. ã‚³ãƒ¡ãƒ³ãƒˆæœ¬æ–‡ (p ã‚¿ã‚°) ã‚’æ¢ã™
                #    (å°‚é–€å®¶ `sc-z8tf0-11`ã€ä¸€èˆ¬ `sc-169yn8p-10` ã«å¯¾å¿œã™ã‚‹ p ã‚¿ã‚°)
                comment_text_tag = comment.find("p", class_=re.compile(r"sc-.*-\d{1,2}$"))
                
                if comment_text_tag:
                    comment_text = comment_text_tag.get_text(strip=True)

                comments_data.append(f"ã€{user_name}ã€‘{comment_text}")

                if len(comments_data) >= 10: # 10ä»¶å–å¾—ã—ãŸã‚‰çµ‚äº†
                    break
            
            if len(comments_data) >= 10:
                break
            
            time.sleep(1) 

        if not comments_data:
            print(f"    - ã‚³ãƒ¡ãƒ³ãƒˆãŒ1ä»¶ã‚‚è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼ˆã¾ãŸã¯ã‚³ãƒ¡ãƒ³ãƒˆæ¬„é–‰é–ï¼‰ã€‚")
            return ["å–å¾—ä¸å¯"] * 10

        if len(comments_data) < 10:
            comments_data.extend(["-"] * (10 - len(comments_data)))

        print(f"    âœ… ã‚³ãƒ¡ãƒ³ãƒˆ {len(comments_data)} ä»¶ã‚’å–å¾—ã—ã¾ã—ãŸã€‚")
        return comments_data[:10]

    except Exception as e:
        print(f"    âŒ ã‚³ãƒ¡ãƒ³ãƒˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        traceback.print_exc()
        return ["å–å¾—ä¸å¯"] * 10
# --- (ä¿®æ­£ã“ã“ã¾ã§) ---


def update_source_sheet(ws, new_articles, existing_urls):
    """
    SOURCE ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ¼ãƒˆã‚’æ›´æ–°ã™ã‚‹ã€‚
    1. æ–°ã—ã„è¨˜äº‹ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    2. æ–°ã—ã„è¨˜äº‹ã‚’ã‚·ãƒ¼ãƒˆã«è¿½åŠ  (A-Eåˆ—)
    3. analysis_flag ãŒ "TRUE" ã‹ã¤ æœ¬æ–‡ãŒç©ºã®è¨˜äº‹ (F-ACåˆ—) ã‚’æ›´æ–°
    """
    
    # --- 1. æ–°ã—ã„è¨˜äº‹ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° ---
    articles_to_add = []
    for article in new_articles:
        if article["url"] not in existing_urls:
            
            post_time = parse_relative_time(article["post_time_str"])
            if post_time:
                post_time_formatted = post_time.strftime("%Y/%m/%d %H:%M:%S")
            else:
                post_time_formatted = article["post_time_str"] 

            row_data = [
                article["keyword"],
                article["url"],
                post_time_formatted,
                article["source"],
                article["title"],
                "TRUE" # Fåˆ—: analysis_flag
            ]
            articles_to_add.append(row_data)
            existing_urls.add(article["url"])

    # --- 2. æ–°ã—ã„è¨˜äº‹ã‚’ã‚·ãƒ¼ãƒˆã«è¿½åŠ  ---
    if articles_to_add:
        try:
            ws.append_rows(articles_to_add, value_input_option="USER_ENTERED")
            print(f"  âœ… {len(articles_to_add)} ä»¶ã®æ–°ã—ã„è¨˜äº‹ã‚’ SOURCEã‚·ãƒ¼ãƒˆ ã«è¿½åŠ ã—ã¾ã—ãŸã€‚")
        except Exception as e:
            print(f"  âŒ æ–°è¦è¨˜äº‹ã®ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã¸ã®æ›¸ãè¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
    else:
        print("  SOURCEã‚·ãƒ¼ãƒˆã«è¿½è¨˜ã™ã¹ãæ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")


    # --- 3. æœ¬æ–‡ãƒ»ã‚³ãƒ¡ãƒ³ãƒˆç­‰ãŒæœªå–å¾—ã®è¨˜äº‹ã‚’æ›´æ–° ---
    try:
        print("  ... æœ¬æ–‡ãƒ»ã‚³ãƒ¡ãƒ³ãƒˆæœªå–å¾—ã®ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‹ã‚‰èª­ã¿è¾¼ã¿ä¸­ ...")
        all_data = ws.get_all_values()
        if len(all_data) <= 1:
            print("  - ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            return 

        headers = all_data[0]
        data_rows = all_data[1:]
        
        # åˆ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ç‰¹å®š (0å§‹ã¾ã‚Š)
        try:
            url_col = headers.index("URL") # Båˆ—
            title_col = headers.index("title") # Eåˆ—
            flag_col = headers.index("analysis_flag") # Fåˆ—
            body_p1_col = headers.index("body_p1") # Gåˆ—
            comment_1_col = headers.index("comment_1") # Såˆ—
        except ValueError as e:
            print(f"  âŒ å¿…è¦ãªåˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {e}ã€‚æœ¬æ–‡å–å¾—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            return

        batch_update_data = []

        # 2è¡Œç›®ã‹ã‚‰ (ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ 0 = 2è¡Œç›®)
        for i, row in enumerate(data_rows):
            row_index = i + 2 # å®Ÿéš›ã®ã‚·ãƒ¼ãƒˆä¸Šã®è¡Œç•ªå·
            
            if len(row) <= max(flag_col, body_p1_col, url_col):
                continue
            
            analysis_flag = row[flag_col]
            body_p1 = row[body_p1_col]
            
            if (analysis_flag.upper() == "TRUE" or analysis_flag == "1") and \
               (not body_p1 or body_p1 == "ï¼ˆæœ¬æ–‡å–å¾—å¤±æ•—ï¼‰"):
                
                title = row[title_col][:30] if len(row) > title_col else "ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ä¸æ˜ï¼‰"
                print(f"  - è¡Œ {row_index} (è¨˜äº‹: {title}...): æœ¬æ–‡(P1-P10)/ã‚³ãƒ¡ãƒ³ãƒˆæ•°/æ—¥æ™‚è£œå®Œ/ã‚³ãƒ¡ãƒ³ãƒˆæœ¬æ–‡ ã‚’å–å¾—ä¸­... (å®Œå…¨å–å¾—)")
                
                article_url = row[url_col]
                article_id_match = re.search(r"/articles/([a-f0-9]+)", article_url)
                if not article_id_match:
                    print(f"    - URLã‹ã‚‰è¨˜äº‹IDãŒæŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ: {article_url}")
                    continue
                
                article_id = article_id_match.group(1)

                article_body_parts, comment_count, full_post_time = get_article_details(article_url)
                
                # (ä¿®æ­£æ¸ˆ) get_yahoo_news_comments ã« article_url ã‚’æ¸¡ã™
                comments_data = get_yahoo_news_comments(article_id, article_url)
                
                update_row_data = []
                update_row_data.extend(article_body_parts) # G-Påˆ— (10åˆ—)
                update_row_data.append(comment_count) # Qåˆ—
                
                if full_post_time:
                    jst = full_post_time.astimezone(timedelta(hours=9))
                    update_row_data.append(jst.strftime("%Y/%m/%d %H:%M:%S"))
                else:
                    update_row_data.append("-") # Råˆ—

                update_row_data.extend(comments_data) # S-ACåˆ— (10åˆ—)
                
                # æ›´æ–°ç¯„å›² (Gåˆ— ã‹ã‚‰ ACåˆ— ã¾ã§)
                start_col_letter = gspread.utils.rowcol_to_a1(row_index, body_p1_col + 1)[0]
                end_col_letter = gspread.utils.rowcol_to_a1(row_index, comment_1_col + 9)
                end_col_letter = ''.join([c for c in end_col_letter if not c.isdigit()])

                range_to_update = f"{start_col_letter}{row_index}:{end_col_letter}{row_index}"
                
                batch_update_data.append({
                    'range': range_to_update,
                    'values': [update_row_data]
                })

                time.sleep(3)
        
        if batch_update_data:
            print(f"  ... {len(batch_update_data)} ä»¶ã®æœ¬æ–‡/ã‚³ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«ä¸€æ‹¬æ›¸ãè¾¼ã¿ä¸­ ...")
            ws.batch_update(batch_update_data, value_input_option="USER_ENTERED")
            print("  âœ… æœ¬æ–‡/ã‚³ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã®ä¸€æ‹¬æ›¸ãè¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

    except Exception as e:
        print(f"  âŒ æœ¬æ–‡ãƒ»ã‚³ãƒ¡ãƒ³ãƒˆå–å¾—ãƒ»æ›¸ãè¾¼ã¿å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        traceback.print_exc()


def sort_and_format_sheet(gc):
    """
    SOURCE ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ¼ãƒˆã® Cåˆ— (æŠ•ç¨¿æ—¥æ™‚) ã®æ›¸å¼ã‚’æ•´ãˆã€
    ã‚·ãƒ¼ãƒˆå…¨ä½“ã‚’ Cåˆ— ã®é™é † (æ–°ã—ã„é †) ã§ã‚½ãƒ¼ãƒˆã™ã‚‹ã€‚
    """
    print("\n===== ğŸ“‘ ã‚¹ãƒ†ãƒƒãƒ—â‘¢ è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã®ã‚½ãƒ¼ãƒˆã¨æ•´å½¢ =====")
    ws = get_worksheet(gc, "SOURCE")
    if not ws:
        return

    try:
        # ã‚·ãƒ¼ãƒˆãŒç©ºã§ãªã„ã‹ç¢ºèª (è¡ŒãŒ1è¡Œ=ãƒ˜ãƒƒãƒ€ãƒ¼ã®ã¿ã€ã¾ãŸã¯0è¡Œã®å ´åˆã‚½ãƒ¼ãƒˆä¸è¦)
        if ws.row_count <= 1:
            print("  - ã‚·ãƒ¼ãƒˆã«ãƒ‡ãƒ¼ã‚¿ãŒãªã„ãŸã‚ã€ã‚½ãƒ¼ãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            return

        print(" ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆä¸Šã§Cåˆ—ã®æ›¸å¼è¨­å®šã¨ã‚½ãƒ¼ãƒˆã‚’å®Ÿè¡Œã—ã¾ã™ã€‚")
        
        # Cåˆ—å…¨ä½“ã®æ›¸å¼è¨­å®šãƒªã‚¯ã‚¨ã‚¹ãƒˆ (C2ã‹ã‚‰Cåˆ—æœ€å¾Œã¾ã§)
        format_request = {
            "repeatCell": {
                "range": {
                    "sheetId": ws.id,
                    "startRowIndex": 1,  # 2è¡Œç›®ã‹ã‚‰ (0-indexed)
                    "endRowIndex": ws.row_count,
                    "startColumnIndex": 2, # Cåˆ— (0-indexed)
                    "endColumnIndex": 3
                },
                "cell": {
                    "userEnteredFormat": {
                        "numberFormat": {
                            "type": "DATE_TIME",
                            "pattern": "yyyy/mm/dd hh:mm:ss"
                        }
                    }
                },
                "fields": "userEnteredFormat.numberFormat"
            }
        }

        # ã‚½ãƒ¼ãƒˆãƒªã‚¯ã‚¨ã‚¹ãƒˆ (Cåˆ—=åˆ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹2 ã§é™é †ã‚½ãƒ¼ãƒˆ)
        sort_request = {
            "sortRange": {
                "range": {
                    "sheetId": ws.id,
                    "startRowIndex": 1, # 2è¡Œç›®ã‹ã‚‰ (ãƒ˜ãƒƒãƒ€ãƒ¼é™¤ã)
                    "endRowIndex": ws.row_count,
                    "startColumnIndex": 0, # Aåˆ—ã‹ã‚‰
                    "endColumnIndex": ws.col_count
                },
                "sortSpecs": [
                    {
                        "dimensionIndex": 2, # Cåˆ— (0-indexed)
                        "sortOrder": "DESCENDING"
                    }
                ]
            }
        }
        
        ws.spreadsheet.batch_update({
            "requests": [format_request, sort_request]
        })
        
        print(f" âœ… Cåˆ—(2è¡Œç›®ã€œ{ws.row_count}è¡Œ) ã®è¡¨ç¤ºå½¢å¼ã‚’ 'yyyy/mm/dd hh:mm:ss' ã«è¨­å®šã—ã¾ã—ãŸã€‚")
        print(" âœ… SOURCEã‚·ãƒ¼ãƒˆã‚’æŠ•ç¨¿æ—¥æ™‚ã®æ–°ã—ã„é †ã«ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆä¸Šã§ä¸¦ã³æ›¿ãˆã¾ã—ãŸã€‚")

    except Exception as e:
        print(f"  âŒ ã‚½ãƒ¼ãƒˆãƒ»æ›¸å¼è¨­å®šä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        traceback.print_exc()


def analyze_with_gemini_and_update_sheet(gc):
    """
    ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã®ã€Œåˆ†æãƒ•ãƒ©ã‚°ã€ãŒç«‹ã£ã¦ã„ã‚‹è¨˜äº‹ï¼ˆæœ€å¤§30ä»¶ï¼‰ã‚’Geminiã§åˆ†æã—ã€
    çµæœã‚’P-Råˆ— (sentiment, category, company_info) ã¨
    AD-AEåˆ— (nissan_mention, nissan_sentiment) ã«ä¸€æ‹¬ã§æ›¸ãè¾¼ã‚€ã€‚
    (ä¿®æ­£æ¸ˆï¼šAPI 429 ã‚¨ãƒ©ãƒ¼å¯¾ç­–ã®ãƒãƒƒãƒå‡¦ç†åŒ–)
    """
    try:
        if not gemini_model:
            print("\n===== ğŸ§  ã‚¹ãƒ†ãƒƒãƒ—â‘£ (ã‚¹ã‚­ãƒƒãƒ—) =====")
            print("  Geminiãƒ¢ãƒ‡ãƒ«ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ãªã„ãŸã‚ã€åˆ†æã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            return

        print("\n===== ğŸ§  ã‚¹ãƒ†ãƒƒãƒ—â‘£ Geminiåˆ†æã®å®Ÿè¡Œãƒ»å³æ™‚åæ˜  (P-R, AD-AEåˆ—) [æœ€å¤§30ä»¶] =====")
        ws = get_worksheet(gc, "SOURCE")
        if not ws:
            return

        print("  ... åˆ†æå¯¾è±¡ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‹ã‚‰èª­ã¿è¾¼ã¿ä¸­ ...")
        all_data = ws.get_all_values()
        if len(all_data) <= 1:
            print("  åˆ†æå¯¾è±¡ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            return

        headers = all_data[0]
        data_rows = all_data[1:]

        # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’å–å¾—ã—ã¦ã€åˆ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å‹•çš„ã«è¦‹ã¤ã‘ã‚‹
        try:
            # å¿…è¦ãªåˆ—ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆ0å§‹ã¾ã‚Šï¼‰ã‚’å–å¾—
            title_col_idx = headers.index("title") + 1 # Eåˆ—
            analysis_flag_col_idx = headers.index("analysis_flag") + 1 # Fåˆ—
            body_col_idx = headers.index("body_p1") + 1 # Gåˆ—
            sentiment_col_idx = headers.index("sentiment") + 1 # Påˆ—
            category_col_idx = headers.index("category") + 1 # Qåˆ—
            company_info_col_idx = headers.index("company_info") + 1 # Råˆ—
            nissan_mention_col_idx = headers.index("nissan_mention") + 1 # ADåˆ—
            nissan_sentiment_col_idx = headers.index("nissan_sentiment") + 1 # AEåˆ—

        except ValueError as e:
            print(f"  âŒ å¿…è¦ãªåˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {e}ã€‚åˆ†æã‚’ä¸­æ–­ã—ã¾ã™ã€‚")
            print(f"  (å–å¾—ã—ãŸãƒ˜ãƒƒãƒ€ãƒ¼: {headers})")
            return
        
        batch_updates = []
        count = 0
        max_analyze = 30 # æœ€å¤§åˆ†æä»¶æ•°

        # 2è¡Œç›®ã‹ã‚‰ (ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹0 = 2è¡Œç›®)
        for i, row in enumerate(data_rows):
            row_index = i + 2 # å®Ÿéš›ã®ã‚·ãƒ¼ãƒˆä¸Šã®è¡Œç•ªå·
            
            if len(row) <= max(analysis_flag_col_idx-1, sentiment_col_idx-1, body_col_idx-1):
                continue

            try:
                analysis_flag = row[analysis_flag_col_idx - 1]
                sentiment = row[sentiment_col_idx - 1]
                
                if (analysis_flag.upper() == "TRUE" or analysis_flag == "1") and (not sentiment or sentiment == "N/A"):
                    
                    if count >= max_analyze:
                        print(f"  åˆ†æä»¶æ•°ãŒ{max_analyze}ä»¶ã«é”ã—ãŸãŸã‚ã€æ®‹ã‚Šã¯æ¬¡å›ã«å›ã—ã¾ã™ã€‚")
                        break
                    
                    count += 1
                    title = row[title_col_idx - 1][:30] # ã‚¿ã‚¤ãƒˆãƒ«åˆ—
                    print(f"  - è¡Œ {row_index} (è¨˜äº‹: {title}...): Geminiåˆ†æã‚’å®Ÿè¡Œä¸­... ({count}/{max_analyze}ä»¶ç›®)")

                    # æœ¬æ–‡ (Gåˆ—ã‹ã‚‰Påˆ—ã®ç›´å‰ã¾ã§)
                    body_p1_to_p10 = row[body_col_idx - 1 : body_col_idx + 9]
                    article_body = " ".join([text for text in body_p1_to_p10 if text and text != "-"])
                    
                    if len(article_body.strip()) < 50: 
                        print(f"    ...æœ¬æ–‡ãŒçŸ­ã™ãã‚‹ãŸã‚ã‚¹ã‚­ãƒƒãƒ— (æœ¬æ–‡: {article_body[:50]}...)")
                        analysis_result = {
                            "sentiment": "N/A (æœ¬æ–‡çŸ­)", "category": "N/A", "company_info": "N/A",
                            "nissan_mention": "-", "nissan_sentiment": "-"
                        }
                    else:
                        analysis_result = analyze_article_with_gemini(article_body)
                    
                    sentiment = analysis_result.get("sentiment", "N/A")
                    category = analysis_result.get("category", "N/A")
                    company_info = analysis_result.get("company_info", "N/A")
                    nissan_mention = analysis_result.get("nissan_mention", "N/A")
                    nissan_sentiment = analysis_result.get("nissan_sentiment", "N/A")

                    # ãƒ¡ã‚¤ãƒ³ã®åˆ†æçµæœ (Påˆ—ã€œRåˆ—)
                    batch_updates.append({
                        'range': f"{gspread.utils.rowcol_to_a1(row_index, sentiment_col_idx)}:{gspread.utils.rowcol_to_a1(row_index, company_info_col_idx)}",
                        'values': [[sentiment, category, company_info]]
                    })
                    
                    # æ—¥ç”£é–¢é€£ã®åˆ†æçµæœ (ADåˆ—ã€œAEåˆ—)
                    batch_updates.append({
                        'range': f"{gspread.utils.rowcol_to_a1(row_index, nissan_mention_col_idx)}:{gspread.utils.rowcol_to_a1(row_index, nissan_sentiment_col_idx)}",
                        'values': [[nissan_mention, nissan_sentiment]]
                    })
                    
                    time.sleep(1) 

            except Exception as e:
                print(f"  âŒ è¡Œ {row_index} ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
                traceback.print_exc()

        if batch_updates:
            print(f"  ... {len(batch_updates) // 2} ä»¶ã®åˆ†æçµæœã‚’ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«ä¸€æ‹¬æ›¸ãè¾¼ã¿ä¸­ ...")
            try:
                ws.batch_update(batch_updates, value_input_option="USER_ENTERED")
                print("  âœ… åˆ†æçµæœã®ä¸€æ‹¬æ›¸ãè¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
            except Exception as e:
                print(f"  âŒ ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã¸ã®ä¸€æ‹¬æ›¸ãè¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
                traceback.print_exc()
        elif count == 0:
            print("  åˆ†æå¯¾è±¡ï¼ˆåˆ†æãƒ•ãƒ©ã‚°ãŒTRUEã§æœªåˆ†æï¼‰ã®è¨˜äº‹ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

    except Exception as e:
        print(f"  âŒ Geminiåˆ†æã‚¹ãƒ†ãƒƒãƒ—å…¨ä½“ã§ã‚¨ãƒ©ãƒ¼: {e}")
        traceback.print_exc()


# (ä¿®æ­£) ãƒ˜ãƒƒãƒ€ãƒ¼è‡ªå‹•è¨­å®šæ©Ÿèƒ½
def check_and_set_headers(ws):
    """
    ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ¼ãƒˆã®1è¡Œç›®ï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼ï¼‰ã‚’ç¢ºèªã—ã€
    å­˜åœ¨ã—ãªã„å ´åˆã‚„ä¸æ•´åˆãŒã‚ã‚‹å ´åˆã«è‡ªå‹•ã§è¨­å®šã™ã‚‹ã€‚
    """
    print("  ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œï¼ˆ1è¡Œç›®ï¼‰ã®æ•´åˆæ€§ã‚’ç¢ºèªä¸­...")
    
    # ãƒ—ãƒ­ã‚°ãƒ©ãƒ ãŒæœŸå¾…ã™ã‚‹ãƒ˜ãƒƒãƒ€ãƒ¼ã®å®Œå…¨ãªãƒªã‚¹ãƒˆ
    expected_headers = [
        'keyword', 'URL', 'post_time_str', 'source', 'title', 'analysis_flag', 
        'body_p1', 'body_p2', 'body_p3', 'body_p4', 'body_p5', 'body_p6', 
        'body_p7', 'body_p8', 'body_p9', 'body_p10', 
        'sentiment', 'category', 'company_info', 
        'comment_count', 'full_post_time', 
        'comment_1', 'comment_2', 'comment_3', 'comment_4', 'comment_5', 
        'comment_6', 'comment_7', 'comment_8', 'comment_9', 'comment_10', 
        'nissan_mention', 'nissan_sentiment'
    ]
    
    try:
        current_headers = ws.row_values(1)
    except GSpreadAPIError as e:
        print(f"  ã‚·ãƒ¼ãƒˆãŒç©ºã®ã‚ˆã†ã§ã™ (ã‚¨ãƒ©ãƒ¼: {e})ã€‚")
        current_headers = []
    except Exception as e:
        print(f"  ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã®èª­ã¿å–ã‚Šã«å¤±æ•—: {e}")
        current_headers = []

    if current_headers != expected_headers:
        print("  ãƒ˜ãƒƒãƒ€ãƒ¼è¡ŒãŒä¸è¶³ã¾ãŸã¯ä¸æ•´åˆã§ã™ã€‚1è¡Œç›®ã«ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’è‡ªå‹•è¨­å®šã—ã¾ã™...")
        try:
            ws.update('A1', [expected_headers], value_input_option='RAW')
            print("  âœ… ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’æ›´æ–°ã—ã¾ã—ãŸã€‚")
            return True
        except Exception as e:
            print(f"  âŒ ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã®è¨­å®šã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            traceback.print_exc()
            return False
    else:
        print("  âœ… ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã¯æ­£å¸¸ã§ã™ã€‚")
        return True


def main():
    """
    ãƒ¡ã‚¤ãƒ³å‡¦ç†
    """
    print("--- çµ±åˆã‚¹ã‚¯ãƒªãƒ—ãƒˆé–‹å§‹ ---")
    start_time = time.time()
    
    # --- ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ— ---
    gc = setup_gspread()
    if not gc:
        print("ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆèªè¨¼ã«å¤±æ•—ã€‚å‡¦ç†ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
        return

    ws = get_worksheet(gc, "SOURCE")
    if not ws:
        print("SOURCE ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ¼ãƒˆã®å–å¾—ã«å¤±æ•—ã€‚å‡¦ç†ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
        return
        
    if not check_and_set_headers(ws):
        print("ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã®è¨­å®šã«å¤±æ•—ã—ãŸãŸã‚ã€å‡¦ç†ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
        return
        
    if not load_prompts():
        print("ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆèª­ã¿è¾¼ã¿ã«å¤±æ•—ã€‚Geminiåˆ†æã¯å®Ÿè¡Œã•ã‚Œã¾ã›ã‚“ã€‚")

    initialize_gemini() # Gemini APIã®åˆæœŸåŒ–

    # --- ã‚¹ãƒ†ãƒƒãƒ—â‘  ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒªã‚¹ãƒˆå–å¾— & ã‚¹ãƒ†ãƒƒãƒ—â‘¡ æœ¬æ–‡ãƒ»ã‚³ãƒ¡ãƒ³ãƒˆå–å¾— ---
    existing_urls = load_existing_urls(ws)
    print(f"  (ç¾åœ¨ {len(existing_urls)} ä»¶ã®è¨˜äº‹URLã‚’ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿)")
    
    for keyword in SEARCH_KEYWORDS:
        print(f"\n===== ğŸ”‘ ã‚¹ãƒ†ãƒƒãƒ—â‘  ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒªã‚¹ãƒˆå–å¾—: {keyword} =====")
        new_articles = get_yahoo_news_search_results(keyword)
        
        print(f"\n===== ğŸ“ ã‚¹ãƒ†ãƒƒãƒ—â‘¡ æœ¬æ–‡/ã‚³ãƒ¡ãƒ³ãƒˆæ›´æ–° (ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {keyword} è¿½åŠ å¾Œ) =====")
        update_source_sheet(ws, new_articles, existing_urls)


    # --- ã‚¹ãƒ†ãƒƒãƒ—â‘¢ ã‚½ãƒ¼ãƒˆ & æ›¸å¼è¨­å®š ---
    sort_and_format_sheet(gc)

    # --- ã‚¹ãƒ†ãƒƒãƒ—â‘£ Gemini åˆ†æ ---
    analyze_with_gemini_and_update_sheet(gc)

    end_time = time.time()
    print(f"\n--- çµ±åˆã‚¹ã‚¯ãƒªãƒ—ãƒˆçµ‚äº† (æ‰€è¦æ™‚é–“: {end_time - start_time:.2f}ç§’) ---")


if __name__ == "__main__":
    main()
